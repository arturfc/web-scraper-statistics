{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LBcf-XJ2EoV"
      },
      "source": [
        "# Run this cell just one time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5sYtmK30mdR",
        "outputId": "0b93a930-0441-4c28-9a2f-a70fd076d58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.3.2\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.3.2)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824003 sha256=0b787455719e7dc115a1523cdebb0336980b8fe39b0d44e70fd991c939503edb\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/d6/52/1178e354ba2207673484f0ccd7b2ded0ab6671ae5c1fc5b49a\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "import re\n",
        "\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.3.2\n",
        "!pip install beautifulsoup4 requests\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, regexp_replace, format_number\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# URL for the Dota 2 heroes list on Gamepedia\n",
        "url = \"https://dota2.gamepedia.com/Heroes\"\n",
        "\n",
        "# Fetch the HTML content from the URL\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Find all hero names within <span> elements with the specified style\n",
        "hero_spans = soup.find_all(\"span\", style=\"font-size:17px; color:white; text-shadow:-1px 0 0.2em black, 0 1px 0.2em black, 1px 0 0.2em black, 0 -1px 0.2em black;\")\n",
        "\n",
        "# Extract hero names and store them in a list\n",
        "hero_names = [span.text.strip() for span in hero_spans]\n",
        "\n",
        "# Add prefix and replace spaces with \"%20\"\n",
        "all_hero_names = [\n",
        "    \"https://dota2protracker.com/hero/\" + urllib.parse.quote(name) + \"#\" for name in hero_names\n",
        "]\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"HeroData\").getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-6qh1pIWVoM"
      },
      "source": [
        "## Pre loading win rate data from all heroes. This cell might take an average of 5 minutes to run. Execute it just one time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV9yMCbXrwCO"
      },
      "outputs": [],
      "source": [
        "all_heroes_data = []\n",
        "\n",
        "for index, url in enumerate(all_hero_names):\n",
        "    # Fetch the HTML content from the URL\n",
        "    response = requests.get(url)\n",
        "    html_content = response.content\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    heroes_data = []\n",
        "\n",
        "    # Find all <tr> elements\n",
        "    hero_rows = soup.find_all(\"tr\", style=\"\")\n",
        "\n",
        "    for row in hero_rows:\n",
        "        hero_data = {}\n",
        "\n",
        "        # Extract hero name from <td class=\"td-hero-pic\">\n",
        "        hero_pic = row.find(\"td\", class_=\"td-hero-pic\")\n",
        "        if hero_pic:\n",
        "            hero_name = hero_pic.a.get(\"title\")\n",
        "            hero_data[\"hero_name\"] = hero_name\n",
        "\n",
        "        # Extract win rates from <td class=\"td-record\">\n",
        "        win_rate_elems = row.find_all(\"td\", class_=\"td-record\")\n",
        "        win_rates = []\n",
        "        for elem in win_rate_elems:\n",
        "            win_rate_span = elem.find(\"span\", class_=[\"green\", \"red\"])\n",
        "            if win_rate_span:\n",
        "                win_rates.append(win_rate_span.text)\n",
        "\n",
        "        if len(win_rates) >= 2:\n",
        "            #hero_data[\"win_rate_1\"] = win_rates[0]\n",
        "            hero_data[\"win_rate_2\"] = win_rates[1]\n",
        "\n",
        "        heroes_data.append(hero_data)\n",
        "\n",
        "    # Removing empty items from the list\n",
        "    heroes_data = [item for item in heroes_data if item]\n",
        "\n",
        "    # Append the heroes_data for this URL to the all_heroes_data list\n",
        "    all_heroes_data.append(heroes_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDIK5vVJ8lBC"
      },
      "outputs": [],
      "source": [
        "def best_heroes_to_draft(input_heroes, analysis_mode):\n",
        "\n",
        "  # List of hero URLs\n",
        "  hero_list = []\n",
        "  hero_list_index = []\n",
        "\n",
        "  # Iterate through the input hero names\n",
        "  for index, input_hero in enumerate(input_heroes):\n",
        "      # Convert the input hero name to a standardized format for comparison\n",
        "      standardized_input_hero = input_hero.lower().replace(\" \", \"%20\")\n",
        "\n",
        "      # Find matches in all_hero_names and store the index of the match\n",
        "      matches = [(i, link) for i, link in enumerate(all_hero_names) if standardized_input_hero in link.lower()]\n",
        "\n",
        "      # Append matches to the hero_list list and store the indices in hero_list_index\n",
        "      for match_index, match_link in matches:\n",
        "          hero_list.append(match_link)\n",
        "          hero_list_index.append(match_index)\n",
        "\n",
        "  # Check if the lengths of input_heroes and all_hero_names are equal\n",
        "  if len(input_heroes) != len(hero_list):\n",
        "    print(hero_list)\n",
        "    assert len(input_heroes) == len(hero_list), \"Lengths of input_heroes and all_hero_names are not equal.\"\n",
        "\n",
        "  display_cleaned_hero_list = [re.sub(r'.*/', '', hero).replace('%20', '_').replace('%27', '').replace('#', '').upper() for hero in hero_list]\n",
        "\n",
        "  #display hero names that will be written as columns on the final table\n",
        "  # display_cleaned_hero_list\n",
        "\n",
        "  for i, index in enumerate(hero_list_index):\n",
        "    heroes_data = all_heroes_data[index]\n",
        "    print(heroes_data)\n",
        "    df = spark.createDataFrame(heroes_data)\n",
        "    #df = df.withColumn(\"win_rate_1\", (regexp_replace(col(\"win_rate_1\"), \"%\", \"\").cast(FloatType()) / 100))\n",
        "    df = df.withColumn(\"win_rate_2\", (regexp_replace(col(\"win_rate_2\"), \"%\", \"\").cast(FloatType()) / 100))\n",
        "    #df = df.withColumn(\"win_rate_1\", format_number(col(\"win_rate_1\"), 3))\n",
        "    df = df.withColumn(\"win_rate_2\", format_number(col(\"win_rate_2\"), 3))\n",
        "\n",
        "    # Create a temporary view for each hero data array\n",
        "    df.createOrReplaceTempView(f\"hero_{i}\")\n",
        "\n",
        "  # Generate the SQL query for creating the selected_heroes view\n",
        "  union_queries = \"\\n    UNION\\n    \".join([\n",
        "      f\"(SELECT hero_name FROM hero_{i})\" # Add WHERE win_rate_2 < .5 ORDER BY win_rate_2 ASC if you want the old version\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  # Generate the main SQL query\n",
        "  select_queries = \",\\n    \".join([\n",
        "      f\"b{i}.win_rate_2 AS {hero_name}\"\n",
        "      for i, hero_name in enumerate(display_cleaned_hero_list)\n",
        "  ])\n",
        "\n",
        "  aggregate_queries = \" + \".join([\n",
        "      f\"b{i}.win_rate_2\"\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  left_join_queries = \"\\n  \".join([\n",
        "      f\"LEFT JOIN hero_{i} b{i} ON a.hero_name = b{i}.hero_name\"\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  where_conditions = \"\\n    AND \".join([\n",
        "      f\"b{i}.win_rate_2 IS NOT NULL\"\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  # Initialize a dictionary to map analysis modes to their respective values\n",
        "  analysis_modes = {\n",
        "      'pos1': ('', '', '--', '--'),\n",
        "      'pos2': ('', '--', '', '--'),\n",
        "      'draft': ('--', '--', '--', ''),\n",
        "  }\n",
        "\n",
        "  # Check if the analysis_mode is valid\n",
        "  if analysis_mode in analysis_modes:\n",
        "      score_restriction, pos1_analysis, pos2_analysis, draft_analysis = analysis_modes[analysis_mode]\n",
        "  else:\n",
        "    raise ValueError(\"Invalid analysis_mode. Choose from 'pos1', 'pos2', or 'draft'.\")\n",
        "\n",
        "  sql_query = f'''\n",
        "    SELECT *\n",
        "    FROM (\n",
        "      SELECT\n",
        "        UPPER(a.hero_name)                                                 AS POTENTIAL_HERO,\n",
        "        ROUND(({aggregate_queries}) / {len(display_cleaned_hero_list)}, 3) AS LOSS_PROBABILITY_SCORE,\n",
        "        {select_queries}\n",
        "      FROM (\n",
        "        {union_queries}\n",
        "      ) a\n",
        "      {left_join_queries}\n",
        "      WHERE {where_conditions}\n",
        "      ORDER BY LOSS_PROBABILITY_SCORE ASC\n",
        "    )\n",
        "    WHERE 1=1\n",
        "      {score_restriction}AND LOSS_PROBABILITY_SCORE < 0.5\n",
        "      {pos1_analysis}AND POTENTIAL_HERO IN (\"RAZOR\", \"BRISTLEBACK\", \"LONE DRUID\", \"NAGA SIREN\", \"GYROCOPTER\", \"SLARK\", \"CHAOS KNIGHT\", \"MONKEY KING\", \"URSA\", \"RIKI\", \"WEAVER\", \"ANTI-MAGE\", \"JUGGERNAUT\", \"MAGNUS\", \"LUNA\", \"SNIPER\", \"ARC WARDEN\", \"DROW RANGER\", \"TROLL WARLORD\", \"MORPHLING\", \"PHANTOM ASSASSIN\", \"PHANTOM LANCER\", \"TEMPLAR ASSASSIN\", \"SVEN\", \"TERRORBLADE\", \"WRAITH KING\", \"SPECTRE\", \"MUERTA\", \"MEDUSA\", \"LIFESTEALER\", \"SHADOW FIEND\")\n",
        "      {pos2_analysis}AND POTENTIAL_HERO IN (\"MAGNUS\", \"PRIMAL BEAST\", \"MUERTA\", \"PANGOLIER\", \"INVOKER\", \"EMBER SPIRIT\", \"QUEEN OF PAIN\", \"PUCK\", \"TEMPLAR ASSASSIN\", \"LINA\", \"MONKEY KING\", \"ZEUS\", \"VOID SPIRIT\", \"OUTWORLD DESTROYER\", \"TINKER\", \"SHADOW FIEND\", \"NECROPHOS\", \"TIMBERSAW\", \"WINDRANGER\", \"HUSKAR\", \"LESHRAC\", \"BATRIDER\", \"DEATH PROPHET\", \"KUNKKA\", \"SNIPER\", \"BROODMOTHER\", \"ARC WARDEN\", \"STORM SPIRIT\")\n",
        "      {draft_analysis}AND POTENTIAL_HERO IN (\"SILENCER\", \"ALCHEMIST\", \"PRIMAL BEAST\", \"GRIMSTROKE\", \"NECROPHOS\")\n",
        "  '''\n",
        "\n",
        "  # Execute the SQL query\n",
        "  spark.sql(sql_query).show()\n",
        "\n",
        "  if(analysis_mode == \"draft\"):\n",
        "    score_loss = spark.sql(f'''SELECT AVG(LOSS_PROBABILITY_SCORE) FROM ({sql_query})''').collect()[0][0]\n",
        "    # input_heroes is being considered as Radiant\n",
        "    if (score_loss > .5):\n",
        "      print(f\"RADIANT team will WIN: {score_loss}\")\n",
        "    else:\n",
        "      print(f\"DIRE team will WIN: {score_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5G8-FUufGWvZ",
        "outputId": "0c66fc5f-2698-4a4a-c75e-414d9cbda991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c6bc6dbf95c9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Available Analysys mode options: \"pos1\", \"pos2\", \"draft\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbest_heroes_to_draft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_heroes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'draft'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-dc81c6041590>\u001b[0m in \u001b[0;36mbest_heroes_to_draft\u001b[0;34m(input_heroes, analysis_mode)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mheroes_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_heroes_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheroes_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheroes_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#df = df.withColumn(\"win_rate_1\", (regexp_replace(col(\"win_rate_1\"), \"%\", \"\").cast(FloatType()) / 100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"win_rate_2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"win_rate_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \"\"\"\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0minfer_dict_as_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferDictAsStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
          ]
        }
      ],
      "source": [
        "# Input hero names\n",
        "input_heroes = [\n",
        "                \"LUNA\",\n",
        "                \"SPIRIT B\",\n",
        "                \"CENTAUR\",\n",
        "                \"SHADOW D\",\n",
        "                \"PHOENIX\"\n",
        "                ]\n",
        "\n",
        "# Available Analysys mode options: \"pos1\", \"pos2\", \"draft\"\n",
        "best_heroes_to_draft(input_heroes, analysis_mode = 'draft')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}