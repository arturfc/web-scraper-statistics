{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17D5hoBlzsjc",
        "outputId": "6e616cb6-980b-428d-b5f8-3f5f82644477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.3.2\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.3.2)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824005 sha256=b2a4d07d043ac9fb5dfbd38eb5006c0b13d37a942ce9e66426c75f9613a0ef2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/d6/52/1178e354ba2207673484f0ccd7b2ded0ab6671ae5c1fc5b49a\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote\n",
        "from urllib.parse import urlparse, unquote\n",
        "import re\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.3.2\n",
        "!pip install beautifulsoup4 requests\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, regexp_replace, format_number\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"HeroData\").getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IRDWE0CW6_sf"
      },
      "outputs": [],
      "source": [
        "# URL for the Dota 2 heroes list on Gamepedia\n",
        "url = \"https://dota2.gamepedia.com/Heroes\"\n",
        "\n",
        "# Fetch the HTML content from the URL\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Find all hero names within <span> elements with the specified style\n",
        "hero_spans = soup.find_all(\"span\", style=\"font-size:17px; color:white; text-shadow:-1px 0 0.2em black, 0 1px 0.2em black, 1px 0 0.2em black, 0 -1px 0.2em black;\")\n",
        "\n",
        "# Extract hero names and store them in a list\n",
        "hero_names = [span.text.strip() for span in hero_spans]\n",
        "\n",
        "all_hero_names = [\n",
        "    \"https://dota2protracker.com/hero/\" + quote(name) + \"/new\" for name in hero_names\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cgC8evAN9-6B"
      },
      "outputs": [],
      "source": [
        "analysis_modes = {\n",
        "    'pos1': \"pos 1,carry,core\",\n",
        "    'pos2': \"pos 2,mid,core\",\n",
        "    'pos3': \"pos 3,offlane,core\",\n",
        "    'pos4': \"pos 4,support\",\n",
        "    'pos5': \"pos 5,support\"\n",
        "}\n",
        "\n",
        "roles = [\n",
        "    \"th-pos-1\",\n",
        "    \"th-pos-2\",\n",
        "    \"th-pos-3\",\n",
        "    \"th-pos-4\",\n",
        "    \"th-pos-5\"\n",
        "]\n",
        "\n",
        "def get_role_by_input_parameter(input_parameter):\n",
        "    try:\n",
        "        role = roles[input_parameter - 1]\n",
        "        return role\n",
        "    except IndexError:\n",
        "        return \"Invalid input_parameter. Please choose a valid role.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "u-614zT91KwZ"
      },
      "outputs": [],
      "source": [
        "def best_heroes_to_draft(input_heroes, analysis_mode, minimum_matches):\n",
        "\n",
        "  position = analysis_modes.get(analysis_mode)\n",
        "\n",
        "  print(\"Correlating input heroes with their pages...\")\n",
        "  # List of hero URLs from dota2protracker\n",
        "  hero_list = []\n",
        "  hero_list_index = []\n",
        "\n",
        "  # Iterate through the input hero names\n",
        "  for index, input_hero in enumerate(input_heroes):\n",
        "      # Convert the input hero name to a standardized format for comparison\n",
        "      standardized_input_hero = input_hero[0].lower().replace(\" \", \"%20\")\n",
        "\n",
        "      # Find matches in all_hero_names and store the index of the match\n",
        "      matches = [(i, link) for i, link in enumerate(all_hero_names) if standardized_input_hero in link.lower()]\n",
        "\n",
        "      # Append matches to the hero_list list and store the indices in hero_list_index\n",
        "      for match_index, match_link in matches:\n",
        "          hero_list.append(match_link)\n",
        "          hero_list_index.append(match_index)\n",
        "\n",
        "  # Check if the lengths of input_heroes and all_hero_names are equal\n",
        "  if len(input_heroes) != len(hero_list):\n",
        "      print(hero_list)\n",
        "      assert len(input_heroes) == len(hero_list), \"Lengths of input_heroes and all_hero_names are not equal.\"\n",
        "\n",
        "  display_cleaned_hero_list = [unquote(urlparse(hero).path.split('/')[-2]).replace('%20', '_').replace('%27', '').replace('#', '').replace(' ', '_').replace('-', '_').replace(\"'\", '').upper() for hero in hero_list]\n",
        "\n",
        "  #WebScrap data and generate pyspark dataframe\n",
        "  print(\"Web scraping data into pyspark dataframe...\")\n",
        "\n",
        "  for index, url in enumerate(hero_list):\n",
        "\n",
        "    # Fetch the HTML content from the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    html_content = response.content\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    heroes_data = []\n",
        "\n",
        "    # Find the div with the corresponding id role and class \"top-heroes-box\"\n",
        "    hero_rows = soup.find(id = get_role_by_input_parameter(input_heroes[index][1]), class_=\"top-heroes-box\")\n",
        "\n",
        "    # Find all divs with the specified class within the previously found div\n",
        "    hero_rows_filtered = hero_rows.find_all(class_=\"flex py-1 px-2 bg-d2pt-gray-3 justify-start border-solid border-b border-d2pt-gray-5\")\n",
        "\n",
        "    # Iterate through each found div\n",
        "    for row in hero_rows_filtered:\n",
        "        # Extract the values of data-hero, data-wr, and data-pos attributes\n",
        "        data_hero = row.get('data-hero')\n",
        "        data_wr = row.get('data-wr')\n",
        "        data_pos = row.get('data-pos')\n",
        "        data_matches = row.get('data-matches')\n",
        "\n",
        "        #Filtering by number of matches and corresponding role you want to play\n",
        "        if int(data_matches) > minimum_matches and data_pos == position:\n",
        "            heroes_data.append({'data_hero': data_hero, 'data_wr': data_wr})\n",
        "\n",
        "    assert len(heroes_data) > 0, f\"No data found on {input_heroes[index][0]}'s choosen role. Try setting it's popular role instead.\"\n",
        "    print(url, \"Done.\")\n",
        "\n",
        "    df = spark.createDataFrame(heroes_data)\n",
        "\n",
        "    df = df.withColumn(\"data_wr\", (regexp_replace(col(\"data_wr\"), \"%\", \"\").cast(FloatType()) / 100))\n",
        "    df = df.withColumn(\"data_wr\", format_number(col(\"data_wr\"), 3))\n",
        "\n",
        "    df.createOrReplaceTempView(f\"hero_{index}\")\n",
        "\n",
        "  print(\"Generating SQL query...\")\n",
        "  # Generate the SQL query for creating the selected_heroes view\n",
        "  union_queries = \"\\n    UNION\\n    \".join([\n",
        "      f\"(SELECT data_hero FROM hero_{i})\" # Add WHERE data_wr < .5 ORDER BY data_wr ASC if you want the old version\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  # Generate the main SQL query\n",
        "  select_queries = \",\\n    \".join([\n",
        "      f\"b{i}.data_wr AS {data_hero}\"\n",
        "      for i, data_hero in enumerate(display_cleaned_hero_list)\n",
        "  ])\n",
        "\n",
        "  aggregate_queries = \" + \".join([\n",
        "      f\"b{i}.data_wr\"\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  left_join_queries = \"\\n  \".join([\n",
        "      f\"LEFT JOIN hero_{i} b{i} ON a.data_hero = b{i}.data_hero\"\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  where_conditions = \"\\n    AND \".join([\n",
        "      f\"b{i}.data_wr IS NOT NULL\"\n",
        "      for i in range(len(display_cleaned_hero_list))\n",
        "  ])\n",
        "\n",
        "  sql_query = f'''\n",
        "    SELECT *\n",
        "    FROM (\n",
        "      SELECT\n",
        "        UPPER(a.data_hero)                                                 AS POTENTIAL_HERO,\n",
        "        ROUND(({aggregate_queries}) / {len(display_cleaned_hero_list)}, 3) AS LOSS_PROBABILITY_SCORE,\n",
        "        {select_queries}\n",
        "      FROM (\n",
        "        {union_queries}\n",
        "      ) a\n",
        "      {left_join_queries}\n",
        "      WHERE {where_conditions}\n",
        "      ORDER BY LOSS_PROBABILITY_SCORE ASC\n",
        "    )\n",
        "    WHERE 1=1\n",
        "      --AND LOSS_PROBABILITY_SCORE < 0.5\n",
        "  '''\n",
        "\n",
        "  # Execute the SQL query\n",
        "  spark.sql(sql_query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KBW_PfgUNbK",
        "outputId": "1181741b-15e5-43f7-c7c9-38f4349da892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlating input heroes with their pages...\n",
            "Web scraping data into pyspark dataframe...\n",
            "https://dota2protracker.com/hero/Necrophos/new Done.\n",
            "https://dota2protracker.com/hero/Templar%20Assassin/new Done.\n",
            "https://dota2protracker.com/hero/Wraith%20King/new Done.\n",
            "https://dota2protracker.com/hero/Phoenix/new Done.\n",
            "https://dota2protracker.com/hero/Grimstroke/new Done.\n",
            "Generating SQL query...\n",
            "+-----------------+----------------------+---------+----------------+-----------+-------+----------+\n",
            "|   POTENTIAL_HERO|LOSS_PROBABILITY_SCORE|NECROPHOS|TEMPLAR_ASSASSIN|WRAITH_KING|PHOENIX|GRIMSTROKE|\n",
            "+-----------------+----------------------+---------+----------------+-----------+-------+----------+\n",
            "|     CHAOS KNIGHT|                 0.361|    0.083|           0.367|      0.469|  0.470|     0.414|\n",
            "|       LONE DRUID|                 0.446|    0.286|           0.578|      0.460|  0.473|     0.432|\n",
            "|CENTAUR WARRUNNER|                 0.451|    0.357|           0.489|      0.455|  0.455|     0.501|\n",
            "|            PUDGE|                 0.452|    0.500|           0.389|      0.556|  0.316|     0.500|\n",
            "|           KUNKKA|                 0.469|    0.600|           0.400|      0.414|  0.452|     0.481|\n",
            "|      BRISTLEBACK|                 0.542|    0.737|           0.509|      0.465|  0.486|     0.514|\n",
            "+-----------------+----------------------+---------+----------------+-----------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Input hero names\n",
        "input_heroes = [\n",
        "                [\"necro\", 1],\n",
        "                [\"templar\", 2],\n",
        "                [\"wraith\", 3],\n",
        "                [\"phoenix\", 4],\n",
        "                [\"grim\", 5]\n",
        "               ]\n",
        "\n",
        "# Available Analysys mode options: \"pos1\", \"pos2\", \"pos3\", \"pos4\", \"pos5\"\n",
        "best_heroes_to_draft(input_heroes, analysis_mode = 'pos3', minimum_matches = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fyc8UbOZcK1U"
      },
      "outputs": [],
      "source": [
        "#TODO: make it get only heroes with matches > 15 but if it's not possible, set it at minimum and specifies what heroes has this condition\n",
        "#TODO: improve performance to atleast 10s (web scraping is taking longer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cEN4f04aW3CT",
        "outputId": "f08f5ce2-1316-4348-9fe7-4ce7ea546efb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://dota2.gamepedia.com/Heroes'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 44
        },
        "id": "M9oUQB1bc6Ez",
        "outputId": "0bc01664-8264-47a2-9d86-2bfc47d1fefe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>data_hero</th><th>data_wr</th></tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "+---------+-------+\n",
              "|data_hero|data_wr|\n",
              "+---------+-------+\n",
              "+---------+-------+"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.sql('''\n",
        "select * from hero_0 WHERE DATA_HERO LIKE \"%TREANT%\"\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "RrW0k0N_EfKJ",
        "outputId": "e2730216-4365-4f72-a617-9905591abf8a"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-efbd27192b68>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spark.sql('''\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mselect\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhero_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0munion\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mselect\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhero_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munion\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: hero_1; line 4 pos 21;\n'Union false, false\n:- 'Union false, false\n:  :- 'Union false, false\n:  :  :- Aggregate [count(1) AS count(1)#55L]\n:  :  :  +- SubqueryAlias hero_0\n:  :  :     +- View (`hero_0`, [data_hero#0,data_wr#7])\n:  :  :        +- Project [data_hero#0, format_number(data_wr#4, 3) AS data_wr#7]\n:  :  :           +- Project [data_hero#0, (cast(cast(regexp_replace(data_wr#1, %, , 1) as float) as double) / cast(100 as double)) AS data_wr#4]\n:  :  :              +- LogicalRDD [data_hero#0, data_wr#1], false\n:  :  +- 'Aggregate [unresolvedalias(count(1), None)]\n:  :     +- 'UnresolvedRelation [hero_1], [], false\n:  +- 'Aggregate [unresolvedalias(count(1), None)]\n:     +- 'UnresolvedRelation [hero_2], [], false\n+- 'Aggregate [unresolvedalias(count(1), None)]\n   +- 'UnresolvedRelation [hero_3], [], false\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''\n",
        "select count(*) from hero_0\n",
        "union all\n",
        "select count(*) from hero_1\n",
        "union all\n",
        "select count(*) from hero_2\n",
        "union all\n",
        "select count(*) from hero_3\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHrISAnPROhz"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
